{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V3.3_dcgan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Hbw-GRN-lbpO",
        "irM5oz84wmvE",
        "zhLIHTlkwvBw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbw-GRN-lbpO",
        "colab_type": "text"
      },
      "source": [
        "### Steps to load Models, make predictions and view predictions\n",
        "\n",
        "Step 1- Run the first 12 code cells of the notebook.\n",
        "\n",
        "Step 2- Go to the last code cell of the notebook. This cell is to load model weights. \n",
        "\n",
        "Step 3- Go to second last code cell of the notebook. This cell is to make predictions and load them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQZ-wrpwqkpw",
        "colab_type": "text"
      },
      "source": [
        "Step 1 note: Don't forget to change pickle paths. This is the path your dataset is loaded from.\n",
        "\n",
        "Step 2 note: Don't forget to modify \"model_path\" variable so you can use the shared model weight files. This is the path where your model weights reside in.\n",
        "\n",
        "Step 3 Note: Don't forget to modify \"plot_path\" variable. This variable determines where to save the plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irM5oz84wmvE",
        "colab_type": "text"
      },
      "source": [
        "### Links of the shared data and weight files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyCtnIYcq7b1",
        "colab_type": "text"
      },
      "source": [
        "__________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huEojNoSq0Y_",
        "colab_type": "text"
      },
      "source": [
        "Links of the shared Weight Files:\n",
        "\n",
        "250 epochs:\n",
        "\n",
        "\thttps://drive.google.com/drive/folders/1xUVIkAGRHxXUhNj-V2-q6M7pnXSn_V4e?usp=sharing\n",
        "\n",
        "\n",
        "500 epochs:\n",
        "\n",
        "\thttps://drive.google.com/drive/folders/1nPecL3pfFkPDTQTnz3PofWWtdk-u3IZF?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyKlsoCarAE3",
        "colab_type": "text"
      },
      "source": [
        "______________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EcADONlrCif",
        "colab_type": "text"
      },
      "source": [
        "Links of the shared Pickle Files:\n",
        "\n",
        "\tHalf Faces:\n",
        "  \n",
        "\t\tGround Truths:\n",
        "\n",
        "\t\t\thttps://drive.google.com/file/d/1-JVnG_wVJR3VgAwi6-Hhu2C-ZAyQ2-_9/view?usp=sharing\n",
        "\n",
        "\t\tOccluded Images:\n",
        "\t\t\thttps://drive.google.com/file/d/1-7E0x-UGFjotUH8UJAWruM9Y0rwEzYzV/view?usp=sharing\n",
        "\n",
        "\n",
        "\n",
        "\tUnrestricted Occlusions:\n",
        "\n",
        "\t\tGround Truths:\n",
        "\n",
        "\t\t\thttps://drive.google.com/file/d/19li26wV60jhrf8UtUhGH6xuocDqiHqPG/view?usp=sharing\n",
        "\n",
        "\t\tOccluded Images:\n",
        "\n",
        "\t\t\thttps://drive.google.com/file/d/179YgtbT7A0YFJsQyFULbQPgPZzdmnySA/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhLIHTlkwvBw",
        "colab_type": "text"
      },
      "source": [
        "### The Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoWfF1Z226T-",
        "colab_type": "text"
      },
      "source": [
        "Step 1- Run the first 12 code cells\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bLWZ7uNEgwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout, LeakyReLU, Conv2DTranspose, ReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "\n",
        "from keras import initializers\n",
        "\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # fraction of memory\n",
        "config.gpu_options.visible_device_list = \"0\"\n",
        "\n",
        "set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQWow1vnIbGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-UfS4sk5sC",
        "colab_type": "text"
      },
      "source": [
        "Step 1 note: Don't forget to change pickle paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bEPxtcJISlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path = '/content/drive/My Drive/biometrics_data/'\n",
        "xname = '1_half_face_occluded.pickle'\n",
        "yname = '1_half_face_labels.pickle'\n",
        "pickle_in = open(os.path.join(path,xname),\"rb\")\n",
        "x = pickle.load(pickle_in)\n",
        "\n",
        "pickle_in = open(os.path.join(path,yname),\"rb\")\n",
        "y = pickle.load(pickle_in)\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTpaol_6KMRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I use this because with 200,200 images, I exceed GPU memory.\n",
        "\n",
        "from skimage.transform import resize\n",
        "x = resize(x, (len(x),64,64,1), anti_aliasing=False)\n",
        "y = resize(y, (len(y),64,64,1), anti_aliasing=False)\n",
        "\n",
        "# Print the shape after resize\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "\n",
        "#Draw the image to be sure occluded image is the same as the ground truth one\n",
        "fig=plt.figure(figsize=(6, 6))\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.imshow(x[1,:,:,0],cmap=\"gray\")\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.imshow(y[1,:,:,0],cmap=\"gray\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWxHj7P1djZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#I print pixel values to be sure occluded and ground truth images are aligned\n",
        "print(x[0,0:5,0:5,0])\n",
        "print(y[0,0:5,0:5,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFe3-3yPEldc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is the function to create a generator. I also use the function and create a generator in the same cell.\n",
        "def creategen():\n",
        "  generator = Sequential()\n",
        "\n",
        "\n",
        "  generator.add(Conv2D(64, (5,5) , strides = (2,2), input_shape = x.shape[1:] , padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(ReLU())  \n",
        "  generator.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "  generator.add(Conv2D(128, (5,5) ,  strides = (2,2),padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(ReLU())  \n",
        "  generator.add(Dropout(0.3))\n",
        "\n",
        "  \n",
        "  generator.add(Conv2D(256, (5,5) ,  strides = (2,2), padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(ReLU())  \n",
        "  generator.add(Dropout(0.3))\n",
        "\n",
        "  # I would use these if there was a bottleneck in the network.\n",
        "  #generator.add(Flatten())\n",
        "  \n",
        "  #generator.add(Dense(64)) \n",
        "  #generator.add(BatchNormalization())\n",
        "  #generator.add(ReLU(alpha=0.2))\n",
        "  \n",
        "  #generator.add(Dense(8*8*128)) \n",
        "  #generator.add(BatchNormalization())\n",
        "  #generator.add(ReLU(alpha=0.2))\n",
        "  \n",
        "  #generator.add(layers.Reshape((8,8,128)))\n",
        "\n",
        "\n",
        "  generator.add(Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(ReLU())\n",
        "  \n",
        "  generator.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "  generator.add(BatchNormalization())\n",
        "  generator.add(ReLU())\n",
        "  \n",
        "  generator.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation = \"tanh\"))\n",
        "  \n",
        "\n",
        "  return generator\n",
        "\n",
        "generator = creategen()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W9TidqzEubv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is the function to create a discriminator. \n",
        "#I also use the function and create a discriminator in the same cell.\n",
        "def createdisc():\n",
        "  discriminator = Sequential()\n",
        "\n",
        "  discriminator.add(Conv2D(64, (5,5) , strides = (2,2), input_shape = x.shape[1:] , padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  discriminator.add(BatchNormalization())\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))  \n",
        "  discriminator.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "  discriminator.add(Conv2D(128, (5,5) ,  strides = (2,2),padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  discriminator.add(BatchNormalization())\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))  \n",
        "  discriminator.add(Dropout(0.3))\n",
        "\n",
        "  \n",
        "  discriminator.add(Conv2D(256, (5,5) ,  strides = (2,2), padding = \"SAME\",kernel_initializer = 'random_normal'))\n",
        "  discriminator.add(BatchNormalization())\n",
        "  discriminator.add(LeakyReLU(alpha=0.2))  \n",
        "  discriminator.add(Dropout(0.3))\n",
        "\n",
        "  \n",
        "  discriminator.add(Flatten())\n",
        "  discriminator.add(Dense(1))\n",
        "  \n",
        "  return discriminator\n",
        "\n",
        "\n",
        "discriminator = createdisc()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmBb2AVRUYek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZadB3ZS4vzoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Learning rate of discriminator is advised to be the double of the generator. \n",
        "# I train the generator two times in each step of training. First is with Gan loss, second is with L2 loss.\n",
        "# That is why I use x4 learning rate in discriminator.\n",
        "\n",
        "opt_disc = Adam(lr=0.00004)\n",
        "discriminator.trainable = True\n",
        "discriminator.compile(loss = \"binary_crossentropy\", optimizer = opt_disc)\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3wLocRlM2fY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = Adam(lr=0.00001)\n",
        "generator.compile(loss='mean_squared_error', optimizer = opt_gen)\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLMiISsSFQyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is the function to create a GAN model. I also use the function and create GAN model in the same cell.\n",
        "\n",
        "def creategan(generator,discriminator):\n",
        "  gan = Sequential()\n",
        "  gan.add(generator)\n",
        "  discriminator.trainable = False\n",
        "  gan.add(discriminator)\n",
        "  return(gan)\n",
        "\n",
        "gan = creategan(generator,discriminator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCaQuj77wZ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gan = Adam(lr=0.00001)\n",
        "gan.compile(loss = \"binary_crossentropy\", optimizer = opt_gan)\n",
        "gan.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi-J96roLL9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is to save models. It saves both the architectures and the weights.\n",
        "# While loading the model, you will only need weights since you declared the architecture in the previous cells.\n",
        "\n",
        "def save_models(gan,discriminator,generator,path,epoch):\n",
        "  datenow = str(datetime.datetime.now().strftime('%m-%d-%H:%M'))\n",
        "  gan.save_weights(os.path.join(path,\"{0}_wgan_{1}.h5\".format(epoch,datenow)))\n",
        "  gan.save(os.path.join(path,\"{0}_mgan_{1}.h5\".format(epoch,datenow)))\n",
        "\n",
        "\n",
        "  discriminator.save_weights(os.path.join(path,\"{0}_wd_{1}.h5\".format(epoch,datenow)))\n",
        "  discriminator.save(os.path.join(path,\"{0}_md_{1}.h5\".format(epoch,datenow)))\n",
        "\n",
        "\n",
        "  generator.save_weights(os.path.join(path,\"{0}_wg_{1}.h5\".format(epoch,datenow)))\n",
        "  generator.save(os.path.join(path,\"{0}_mg_{1}.h5\".format(epoch,datenow)))\n",
        "  \n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpfaOehsWRKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is to obtain test losses in each training step. I use this function in the \"train\" function.\n",
        "def test(x,y):\n",
        "  gan_inp_t = x  \n",
        "  gan_label_t = np.ones([len(gan_inp_t)])\n",
        "  gan_predict_t = None\n",
        "  \n",
        "  disc_inp_t = None\n",
        "\n",
        "\n",
        "  disc_label_t = np.zeros([len(gan_inp_t)*2])\n",
        "  disc_label_t[len(gan_inp_t):] = 1\n",
        "  \n",
        "  disc_predict_t = None\n",
        "  \n",
        "\n",
        "  gen_predict_t = generator.predict(gan_inp_t)\n",
        "\n",
        "\n",
        "  disc_inp_t = np.concatenate((gen_predict_t,y), axis = 0)\n",
        "  disc_predict_t = discriminator.predict(disc_inp_t)\n",
        "\n",
        "  d_loss_t = discriminator.test_on_batch(disc_inp_t,disc_label_t)\n",
        "\n",
        "  gan_loss_t = gan.test_on_batch(gan_inp_t,gan_label_t)\n",
        "  \n",
        "  return (gan_loss_t, d_loss_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vPoV2QlqyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In the previous versions, I used this function to pretrain the discriminator in each training step.\n",
        "# I do not use this function in this version.\n",
        "def pretrain(x,y,nepoch):\n",
        "    \n",
        "  gen_predict = None\n",
        "  \n",
        "  \n",
        "  gan_inp = x  \n",
        "  gan_label = np.ones(64)\n",
        "  gan_predict = None\n",
        "  \n",
        "  disc_inp = None\n",
        "\n",
        "\n",
        "  disc_label = np.zeros(64*2)\n",
        "  disc_label[64:] = 1\n",
        "  \n",
        "  disc_predict = None\n",
        "  \n",
        "  \n",
        "  for epoch in range(nepoch):\n",
        "    for batch_ctr in range(65):\n",
        "\n",
        "\n",
        "      gen_predict = generator.predict(gan_inp[batch_ctr*64:(batch_ctr+1)*64])\n",
        "\n",
        "\n",
        "      disc_inp = np.concatenate((gen_predict,y[batch_ctr*64:(batch_ctr+1)*64]), axis = 0)\n",
        "      disc_predict = discriminator.predict(disc_inp)\n",
        "\n",
        "\n",
        "\n",
        "      d_loss = discriminator.train_on_batch(disc_inp,disc_label)\n",
        "\n",
        "\n",
        "      gan_loss = gan.test_on_batch(gan_inp[batch_ctr*64:(batch_ctr+1)*64],\n",
        "                                    gan_label)\n",
        "    \n",
        "\n",
        "    (tgan,tdisc)= test(x[4160:],y[4160:])\n",
        "    print(\"Pretrain Epoch Gan Loss: {0}       Disc Loss: {1}\".format(gan_loss,d_loss))\n",
        "    print(\"Pretrain Epoch Test Gan Loss: {0}  Test Disc Loss: {1} \\n\\n\\n\".format(tgan,tdisc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiZh2paxOHTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I use isolated mini batches to slow down discriminator:\n",
        "# I also use smooth/noisy labels proposed by Salimans et al 2016\n",
        "# Reference: https://github.com/soumith/ganhacks\n",
        "\n",
        "\n",
        "\n",
        "def train(x,y, nepoch, model_save_path = \"/content/drive/My Drive/biometrics_data/models/dummymodel\"):\n",
        "\n",
        "  gen_predict = None\n",
        "  \n",
        "  #Initialize the inputs and the labels.\n",
        "  gan_inp = x  \n",
        "  gan_label = np.ones(64)\n",
        "  gan_predict = None\n",
        "  \n",
        "  disc_inp = None\n",
        "\n",
        "\n",
        "  disc_label = np.zeros(64*2)\n",
        "  disc_label[64:] = 1\n",
        "  \n",
        "  disc_predict = None\n",
        "  \n",
        "\n",
        "  sess = tf.Session()\n",
        "  \n",
        "\n",
        "  for epoch in range(nepoch):\n",
        "\n",
        "    for batch_ctr in range(65):\n",
        "\n",
        "      #Generator makes a prediction.\n",
        "      gen_predict = generator.predict(gan_inp[batch_ctr*64:(batch_ctr+1)*64])\n",
        "\n",
        "\n",
        "      #Minibatch isolation and label smoothing is done here:\n",
        "      \n",
        "      if(epoch%2==0):\n",
        "        disc_inp = gen_predict\n",
        "        #disc_label = np.zeros(64) //// I would use this line if there was no noisy labels.\n",
        "        disc_label = np.random.normal(loc=0, scale=0.10, size=64)\n",
        "      else:\n",
        "        disc_inp = y[batch_ctr*64:(batch_ctr+1)*64]\n",
        "        #disc_label = np.ones(64)  //// I would use this line if there was no noisy labels.\n",
        "        disc_label = np.random.normal(loc=1, scale=0.10, size=64)\n",
        "\n",
        "\n",
        "        \n",
        "      #Initialize a label variable for generator to use it in training.  \n",
        "      gen_label = y[batch_ctr*64:(batch_ctr+1)*64]\n",
        "      \n",
        "      \n",
        "      #Do one training step. Also assign the losses to variables. We will print them.\n",
        "      d_loss = discriminator.train_on_batch(disc_inp,disc_label)\n",
        "\n",
        "\n",
        "      gan_loss = gan.train_on_batch(gan_inp[batch_ctr*64:(batch_ctr+1)*64],\n",
        "                                    gan_label)\n",
        "\n",
        "      gen_loss = generator.train_on_batch(gan_inp[batch_ctr*64:(batch_ctr+1)*64],gen_label)\n",
        "      \n",
        "    \n",
        "    \n",
        "    if epoch+1%500 == 0:\n",
        "      save_models(gan,discriminator,generator, model_save_path,epoch+1)  \n",
        "      print(\"MODEL SAVED\")\n",
        "      \n",
        "    \n",
        "    # Test images are the images after the 4160th image. It makes 311 test images.\n",
        "    (tgan,tdisc)= test(x[4160:],y[4160:])\n",
        "    print(\"Epoch: {2} Gan Loss: {0}       Disc Loss: {1}        Gen Loss: {3}\".format(gan_loss,d_loss,epoch+1,gen_loss))\n",
        "    print(\"Epoch: {2} Test Gan Loss: {0}  Test Disc Loss: {1} \\n\\n\\n\".format(tgan,tdisc,epoch+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmhHddBiTVjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking lengths of the input and ground truth arrays. Also checking if normalization is done.\n",
        "print(len(x),len(y))\n",
        "print(x.max(),x.min())\n",
        "print(y.max(),y.min())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcWGd0HNqQK2",
        "colab_type": "text"
      },
      "source": [
        "If you would like to make trainings, don't forget to modify \"model_save_path\" variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB3UtK6-1MlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use this cell if you have loaded and viewed the results, and want to train further.\n",
        "# If you start the model from beginning, using above cells where I declare \n",
        "# generator architecture, discriminator architecture etc. , you don't need to use this cell since\n",
        "# I already compile the models in above cells.\n",
        "\n",
        "\n",
        "# Learning rate of discriminator is advised to be the double of the generator. \n",
        "# I train the generator two times in each step of training. First is with Gan loss, second is with L2 loss.\n",
        "# That is why I use x4 learning rate in discriminator.\n",
        "\n",
        "\"\"\"opt_disc = Adam(lr=0.00004)\n",
        "discriminator.trainable = True\n",
        "discriminator.compile(loss = \"binary_crossentropy\", optimizer = opt_disc)\n",
        "discriminator.summary()\n",
        "\n",
        "opt_gen = Adam(lr=0.00001)\n",
        "generator.compile(loss='mean_squared_error', optimizer = opt_gen)\n",
        "generator.summary()\n",
        "\n",
        "opt_gan = Adam(lr=0.00001)\n",
        "gan.compile(loss = \"binary_crossentropy\", optimizer = opt_gan)\n",
        "gan.summary()\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4yECos4QNIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To train, don't forget to:\n",
        "# -compile the models\n",
        "# - run the necessary cells to declare the necessary functions such as \"train\" and \"save_models\" and \"test\"\n",
        "\n",
        "#If you train models from beginning, just run the cells of the notebook in order.\n",
        "\n",
        "#If you want to further train loaded models, use the previous cell to compile models.\n",
        "\n",
        "model_save_path = \"/content/drive/My Drive/biometrics_data/models/half_face\"\n",
        "batch_size = 64\n",
        "train(x,y,504,model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC6S0LiJqWCW",
        "colab_type": "text"
      },
      "source": [
        "Step 3 - Make predictions and view them.\n",
        "\n",
        "Step 3 Note: Don't forget to modify \"plot_path\" variable. This variable determines where to save the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzF8GvhT1_6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Making predictions and drawing them.\n",
        "#First row: Occluded images\n",
        "#Second row: Ground Truth images\n",
        "#Third row: Predictions\n",
        "\n",
        "import datetime\n",
        "plot_path = \"/content/drive/My Drive/biometrics_data/plots/half_face\"\n",
        "\n",
        "a = 4160\n",
        "b = 4170\n",
        "pred=generator.predict(x[a:b])\n",
        "\n",
        "fig = plt.figure(figsize = (20,10))\n",
        "for ctr in range(10):\n",
        "  fig.add_subplot(3,10,ctr+1)\n",
        "  plt.imshow(np.reshape(x[a + ctr],(64,64)),  cmap = \"gray\")\n",
        "\n",
        "  \n",
        "\n",
        "for ctr in range(10):\n",
        "  fig.add_subplot(3,10,(10 + ctr + 1))\n",
        "  plt.imshow(np.reshape(y[a + ctr]/255,(64,64)),  cmap = \"gray\")  \n",
        "\n",
        "\n",
        "for ctr in range(10):\n",
        "  fig.add_subplot(3,10,(20 + ctr + 1))\n",
        "  plt.imshow(np.reshape(pred[ctr],(64,64)),  cmap = \"gray\")\n",
        "  \n",
        "plt.savefig(os.path.join(plot_path,str(datetime.datetime.now().strftime('%m-%d-%H:%M'))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-RX2vblDFf",
        "colab_type": "text"
      },
      "source": [
        "Step 2- Load weights here:\n",
        "(Don't forget to modify the model_path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3OxJnJn5QMLr",
        "colab": {}
      },
      "source": [
        "model_path = \"/content/drive/My Drive/biometrics_data/models/last_experiment_250/\"\n",
        "\n",
        "epoch = 250\n",
        "\n",
        "\n",
        "\n",
        "generator = creategen()\n",
        "generator.load_weights(os.path.join(model_path,\"{0}_wg.h5\".format(epoch)))\n",
        "discriminator = createdisc()\n",
        "discriminator.load_weights(os.path.join(model_path,\"{0}_wd.h5\".format(epoch)))\n",
        "\n",
        "\n",
        "gan = creategan(generator,discriminator)\n",
        "gan.load_weights(os.path.join(model_path,\"{0}_wgan.h5\".format(epoch)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM3E25Ep2zjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}